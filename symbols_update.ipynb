{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68b124b-3d52-406d-b92e-989db0a4dbfa",
   "metadata": {},
   "source": [
    "**Internsip|Summer 2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad0aee-b27c-43fe-b081-7e614d098894",
   "metadata": {},
   "source": [
    "**IMPORTANT**\n",
    "\n",
    "You can use this **Jupyter** notebook *symbols_update.ipynb* to complete the assignment. \n",
    "\n",
    "If you are not familiar with **Jupyter**, please feel free to develop your solution using standard **.py** file instead.\n",
    "\n",
    "To complete the assignment, you will need to **pip install pandas** library.\n",
    "\n",
    "Please upload all your work to public **GitHub** repository and share the link with us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d782eddd-aca1-4a74-aa87-41a2df0581e0",
   "metadata": {},
   "source": [
    "There are three .csv files that we want to upload sequentially to the database: **symbols_update_1.csv**, **symbols_update_2.csv**, and **symbols_update_3.csv**. For the sake of simplicity, the database is represented by another .csv file **database.csv**.\n",
    "\n",
    "The goal it to complete **SymbolsUpdate** class with three functions:\n",
    "- ***load_new_data_from_file*** : loads and processes data from **symbols_update_n.csv** file\n",
    "- ***save_new_data*** : saves loaded data to the **database.csv** file\n",
    "- ***get_data_from_database*** : returns the most recently updated data for every symbol in **database.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e34b5cac-8449-4a7e-80af-76a9be536867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# import key libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d06fef-a47d-414c-9853-ac7fc9f1d57d",
   "metadata": {},
   "source": [
    "### Note on ISIN Handling\n",
    "\n",
    "- ISIN is an International Securities Identification Number, a 12-digit alphanumeric, whhich is a unqiue identifier of a given security.\n",
    "- The Excel file was compiled based on the data from the ISIN [website](https://www.isin.net/country-codes/) (hyperlinked).\n",
    "- This will be used to identify the \"country\" entry in the database.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f812c76-ccec-4d08-8124-a25e0cfdbedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the country codes from the Excel file\n",
    "country_codes_df = pd.read_excel('CounntryCodes.xlsx')\n",
    "\n",
    "# Create the dictionary from the DataFrame for quick lookup\n",
    "country_codes_dict = pd.Series(country_codes_df.Definition.values, index=country_codes_df['Code Value']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9149a756-6189-472d-ab74-1fe47d86f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_letters(isin):\n",
    "    # extract the first two ISIN letters - the country code\n",
    "    country_code = ''\n",
    "    for char in isin:\n",
    "        if char.isalpha():\n",
    "            country_code += char\n",
    "            if len(country_code) == 2:\n",
    "                break\n",
    "\n",
    "    # return the country name or 'Unknown' just in case\n",
    "    return country_codes_dict.get(country_code, 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a78bc57-4811-4c12-be9c-23f0eedb8fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymbolsUpdate:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialisation of the SymbolsUpdate class with the database path.\n",
    "        \"\"\"\n",
    "        self.database_file = 'database.csv'\n",
    "\n",
    "    def load_new_data_from_file(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads, processes data from specified path, returns as a DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        file_path (str): The path to the symbols update CSV file.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: A DataFrame containing the processed data.\n",
    "        \"\"\"\n",
    "        # Initialise a DataFrame\n",
    "        new = pd.DataFrame()\n",
    "\n",
    "        # Read the CSV\n",
    "        file = pd.read_csv(file_path)\n",
    "\n",
    "        # Iterate through each row\n",
    "        for i in range(len(file)):\n",
    "            symbol = file['symbol'][i]\n",
    "            country_id = extract_letters(file['isin'][i])\n",
    "            hold = file['hold'][i]\n",
    "            cusip = file['cusip'][i]\n",
    "            isin = file['isin'][i]\n",
    "            current_time = datetime.datetime.now()\n",
    "            formatted_time = current_time.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "            # Create rows for 'cusip' and 'isin'\n",
    "            row1 = {'symbol': symbol, 'hold': hold, 'country': country_id, 'item': 'cusip', 'item_value': cusip, 'updatedby': agent, 'updatetime': formatted_time}\n",
    "            row2 = {'symbol': symbol, 'hold': hold, 'country': country_id, 'item': 'isin', 'item_value': isin, 'updatedby': agent, 'updatetime': formatted_time}\n",
    "\n",
    "            # Combine the rows\n",
    "            pair = pd.DataFrame([row1, row2])\n",
    "\n",
    "            # Append to DataFrame\n",
    "            new = pd.concat([new, pair], ignore_index=True)\n",
    "\n",
    "        return new\n",
    "\n",
    "    def save_new_data(self, input_data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Adds the provided DataFrame to the database file.\n",
    "\n",
    "        Parameters:\n",
    "        input_data (pd.DataFrame): The DataFrame to be added.\n",
    "        \"\"\"\n",
    "        # Load database\n",
    "        DB = pd.read_csv(self.database_file)\n",
    "\n",
    "        # Check if empty\n",
    "        if DB.empty:\n",
    "            # Save data directly if the database is empty\n",
    "            input_data.to_csv(self.database_file, index=False)\n",
    "        else:\n",
    "            # Concatenate the database with the data save\n",
    "            pd.concat([DB, input_data], ignore_index=True).to_csv(self.database_file, index=False)\n",
    "\n",
    "    def get_data_from_database(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns the up to date data for each symbol from the database file.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: A DataFrame containing the most recent updates for each symbol.\n",
    "        \"\"\"\n",
    "        # Load database\n",
    "        DB = pd.read_csv(self.database_file)\n",
    "\n",
    "        # Sort DataFrame by 'symbol' and 'updatetime'\n",
    "        DB = DB.sort_values(by=['symbol', 'updatetime'], ascending=[True, False])\n",
    "\n",
    "        # Group by 'symbol' and select the most recent rows per symbol\n",
    "        DB = DB.groupby('symbol').head(2)\n",
    "\n",
    "        # Sort in alphabetical order\n",
    "        DB = DB.sort_values(by='symbol')\n",
    "\n",
    "        return DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a18d3-a538-4576-865f-8e6cb8ad733b",
   "metadata": {},
   "source": [
    "Running the model should return the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25466760-3699-4d9c-9b1c-f42537ccbe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'database.csv' was empty and has been initialised.\n"
     ]
    }
   ],
   "source": [
    "initialise_database()\n",
    "\n",
    "# content for the 'updatedby' column\n",
    "agent = 'petroineos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00612d0b-7f3d-4933-b866-b96ae3dd1749",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m su \u001b[38;5;241m=\u001b[39m SymbolsUpdate()\n\u001b[1;32m      2\u001b[0m new_data \u001b[38;5;241m=\u001b[39m su\u001b[38;5;241m.\u001b[39mload_new_data_from_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbols_update_1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43msu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_new_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m new_data \u001b[38;5;241m=\u001b[39m su\u001b[38;5;241m.\u001b[39mload_new_data_from_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbols_update_2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m su\u001b[38;5;241m.\u001b[39msave_new_data(new_data)\n",
      "Cell \u001b[0;32mIn[4], line 54\u001b[0m, in \u001b[0;36mSymbolsUpdate.save_new_data\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mAdds the provided DataFrame to the database file.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03minput_data (pd.DataFrame): The DataFrame to be added.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Load database\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m DB \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Check if empty\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DB\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Save data directly if the database is empty\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "su = SymbolsUpdate()\n",
    "new_data = su.load_new_data_from_file('symbols_update_1.csv')\n",
    "su.save_new_data(new_data)\n",
    "new_data = su.load_new_data_from_file('symbols_update_2.csv')\n",
    "su.save_new_data(new_data)\n",
    "new_data = su.load_new_data_from_file('symbols_update_3.csv')\n",
    "su.save_new_data(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9140e1-e6ed-4788-9d3c-dd7e77e157f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "su.get_data_from_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f29f9-2145-401f-8553-32713ce0f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('output.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87036716-6823-4649-a733-5cd86a57bc20",
   "metadata": {},
   "source": [
    "Please note:  \n",
    "- *isin* and *cusip* columns from **symbols_update_n.csv** files are stored as item/item_value pairs in **database.csv**\n",
    "- *country id* is derived from the *isin* field (e.g. US01222911 is US, GB12222201 is GB, etcâ€¦)\n",
    "- The most recent update for each symbol is returned based on the **updatetime** column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90187aac-7ce3-4191-a7ad-0f3c04bf584b",
   "metadata": {},
   "source": [
    "**Good luck!!!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
